{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data\n",
    "Our data is a text file of English-German sentence pairs. First we will read the file us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "    # open the file\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a text into sentences\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_text(\"deu.txt\")\n",
    "deu_eng = to_lines(data)\n",
    "\n",
    "deu_eng = array(deu_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The actual data contains over 150,000 sentence-pairs. However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. You can change this number as per you system computation power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deu_eng = deu_eng[:50000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "### Text Cleaning\n",
    "Let's take a look at our data, then we will decide which pre-processing steps to adop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Go.', 'Geh.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
       "       ['Hi.', 'Hallo!',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
       "       ['Hi.', 'Grüß Gott!',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
       "       ...,\n",
       "       ['We took a wrong turn.', 'Wir sind falsch abgebogen.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #823901 (jellorage) & #2112094 (freddy1)'],\n",
       "       ['We traveled together.', 'Wir waren zusammen auf Reisen.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600396 (Pfirsichbaeumchen)'],\n",
       "       ['We traveled together.', 'Wir sind zusammen gereist.',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600398 (Pfirsichbaeumchen)']],\n",
       "      dtype='<U537')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
    "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Go', 'Geh',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
       "       ['Hi', 'Hallo',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
       "       ['Hi', 'Grüß Gott',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
       "       ...,\n",
       "       ['We took a wrong turn', 'Wir sind falsch abgebogen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #823901 (jellorage) & #2112094 (freddy1)'],\n",
       "       ['We traveled together', 'Wir waren zusammen auf Reisen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600396 (Pfirsichbaeumchen)'],\n",
       "       ['We traveled together', 'Wir sind zusammen gereist',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600398 (Pfirsichbaeumchen)']],\n",
       "      dtype='<U537')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "for i in range(len(deu_eng)):\n",
    "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
    "    \n",
    "    deu_eng[i,1] = deu_eng[i,1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 'geh',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n",
       "       ['hi', 'hallo',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n",
       "       ['hi', 'grüß gott',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n",
       "       ...,\n",
       "       ['we took a wrong turn', 'wir sind falsch abgebogen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #823901 (jellorage) & #2112094 (freddy1)'],\n",
       "       ['we traveled together', 'wir waren zusammen auf reisen',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600396 (Pfirsichbaeumchen)'],\n",
       "       ['we traveled together', 'wir sind zusammen gereist',\n",
       "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1582121 (Spamster) & #1600398 (Pfirsichbaeumchen)']],\n",
       "      dtype='<U537')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deu_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "To feed our data in a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Before that, let's visualise the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "deu_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in deu_eng[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "\n",
    "for i in deu_eng[:,1]:\n",
    "    deu_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of the German sentences is 11 and that of the English phrases is 8.\n",
    "Let's vectorize our text data by using Keras's Tokenizer() class. It will turn our sentences into sequences of integers. Then we will pad those sequences with zeros to make all the sequences of same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6152\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deutch Vocabulary Size: 10112\n"
     ]
    }
   ],
   "source": [
    "# prepare Deutch tokenizer\n",
    "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
    "\n",
    "deu_length = 8\n",
    "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below is a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "We will now split the data into train and test set for model training and evaluation, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the exciting part! Let us define our Seq2Seq model architecture. We are using an Embedding layer and an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note that we have used 'sparse_categorical_crossentropy' as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory.\n",
    "\n",
    "It seems we are all set to start training our model. We will train it for 30 epochs and with a batch size of 512. You may change and play these hyperparameters. We will also be using ModelCheckpoint() to save the best model with lowest validation loss. I personally prefer this method over early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "34/63 [===============>..............] - ETA: 39s - loss: 3.7428"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ac11aca639b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n\u001b[0m\u001b[0;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'model.h1.24_jan_19'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=30, batch_size=512, \n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "Let's load the saved model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h1.24_jan_19')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)            \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
